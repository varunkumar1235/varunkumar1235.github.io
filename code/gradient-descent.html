<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>GPU Memory Allocation - Knapsack Algorithm</title>
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css"
      rel="stylesheet"
    />
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css"
      rel="stylesheet"
    />
    <style>
      body {
        background: linear-gradient(
          135deg,
          #0f0f23 0%,
          #1a1a2e 50%,
          #16213e 100%
        );
        min-height: 100vh;
        color: #e0e0e0;
      }

      .code-container {
        background: rgba(15, 15, 35, 0.95);
        border-radius: 12px;
        padding: 30px;
        margin: 40px auto;
        max-width: 900px;
        box-shadow: 0 8px 32px rgba(0, 255, 157, 0.1);
        border: 1px solid rgba(0, 255, 157, 0.2);
      }

      .page-title {
        text-align: center;
        color: #00ff9d;
        font-weight: 300;
        margin-bottom: 30px;
        text-shadow: 0 0 20px rgba(0, 255, 157, 0.3);
      }

      .algorithm-description {
        background: rgba(0, 255, 157, 0.1);
        padding: 20px;
        border-radius: 8px;
        margin-bottom: 30px;
        border-left: 4px solid #00ff9d;
      }

      pre {
        background: #1e1e3f !important;
        border-radius: 8px;
        padding: 25px;
        overflow-x: auto;
        border: 1px solid rgba(0, 255, 157, 0.3);
        box-shadow: inset 0 0 20px rgba(0, 0, 0, 0.5);
      }

      code {
        color: #e0e0e0;
        font-family: "Courier New", monospace;
        font-size: 14px;
        line-height: 1.6;
      }

      .back-button {
        background: linear-gradient(45deg, #00ff9d, #00d4aa);
        border: none;
        border-radius: 25px;
        padding: 12px 30px;
        font-size: 16px;
        font-weight: 500;
        text-transform: none;
        box-shadow: 0 4px 15px rgba(0, 255, 157, 0.3);
        transition: all 0.3s ease;
        margin-top: 40px;
      }

      .back-button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(0, 255, 157, 0.4);
        background: linear-gradient(45deg, #00d4aa, #00ff9d);
      }

      .button-container {
        text-align: center;
        margin-top: 50px;
      }

      .complexity-info {
        background: rgba(255, 193, 7, 0.1);
        padding: 15px;
        border-radius: 8px;
        margin-top: 20px;
        border-left: 4px solid #ffc107;
      }

      .highlight {
        color: #00ff9d;
      }

      /* Syntax highlighting overrides */
      .token.keyword {
        color: #ff79c6;
      }
      .token.string {
        color: #f1fa8c;
      }
      .token.comment {
        color: #6272a4;
      }
      .token.number {
        color: #bd93f9;
      }
      .token.function {
        color: #50fa7b;
      }
      .token.operator {
        color: #ff79c6;
      }
      .token.punctuation {
        color: #f8f8f2;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="code-container">
        <pre><code class="language-cpp"># Importing Libraries
import numpy as np
import matplotlib.pyplot as plt

def mean_squared_error(y_true, y_predicted):
    
    # Calculating the loss or cost
    cost = np.sum((y_true-y_predicted)**2) / len(y_true)
    return cost

# Gradient Descent Function
# Here iterations, learning_rate, stopping_threshold
# are hyperparameters that can be tuned
def gradient_descent(x, y, iterations = 1000, learning_rate = 0.0001, 
                     stopping_threshold = 1e-6):
    
    # Initializing weight, bias, learning rate and iterations
    current_weight = 0.1
    current_bias = 0.01
    iterations = iterations
    learning_rate = learning_rate
    n = float(len(x))
    
    costs = []
    weights = []
    previous_cost = None
    
    # Estimation of optimal parameters 
    for i in range(iterations):
        
        # Making predictions
        y_predicted = (current_weight * x) + current_bias
        
        # Calculating the current cost
        current_cost = mean_squared_error(y, y_predicted)

        # If the change in cost is less than or equal to 
        # stopping_threshold we stop the gradient descent
        if previous_cost and abs(previous_cost-current_cost)<=stopping_threshold:
            break
        
        previous_cost = current_cost

        costs.append(current_cost)
        weights.append(current_weight)
        
        # Calculating the gradients
        weight_derivative = -(2/n) * sum(x * (y-y_predicted))
        bias_derivative = -(2/n) * sum(y-y_predicted)
        
        # Updating weights and bias
        current_weight = current_weight - (learning_rate * weight_derivative)
        current_bias = current_bias - (learning_rate * bias_derivative)
                
        # Printing the parameters for each 1000th iteration
        print(f"Iteration {i+1}: Cost {current_cost}, Weight \
        {current_weight}, Bias {current_bias}")
    
    
    # Visualizing the weights and cost at for all iterations
    plt.figure(figsize = (8,6))
    plt.plot(weights, costs)
    plt.scatter(weights, costs, marker='o', color='red')
    plt.title("Cost vs Weights")
    plt.ylabel("Cost")
    plt.xlabel("Weight")
    plt.show()
    
    return current_weight, current_bias


def main():
    
    # Data
    X = np.array([32.50234527, 53.42680403, 61.53035803, 47.47563963, 59.81320787,
           55.14218841, 52.21179669, 39.29956669, 48.10504169, 52.55001444,
           45.41973014, 54.35163488, 44.1640495 , 58.16847072, 56.72720806,
           48.95588857, 44.68719623, 60.29732685, 45.61864377, 38.81681754])
    Y = np.array([31.70700585, 68.77759598, 62.5623823 , 71.54663223, 87.23092513,
           78.21151827, 79.64197305, 59.17148932, 75.3312423 , 71.30087989,
           55.16567715, 82.47884676, 62.00892325, 75.39287043, 81.43619216,
           60.72360244, 82.89250373, 97.37989686, 48.84715332, 56.87721319])

    # Estimating weight and bias using gradient descent
    estimated_weight, estimated_bias = gradient_descent(X, Y, iterations=2000)
    print(f"Estimated Weight: {estimated_weight}\nEstimated Bias: {estimated_bias}")

    # Making predictions using estimated parameters
    Y_pred = estimated_weight*X + estimated_bias

    # Plotting the regression line
    plt.figure(figsize = (8,6))
    plt.scatter(X, Y, marker='o', color='red')
    plt.plot([min(X), max(X)], [min(Y_pred), max(Y_pred)], color='blue',markerfacecolor='red',
             markersize=10,linestyle='dashed')
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.show()

    
if __name__=="__main__":
    main()
</code></pre>
      </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
  </body>
</html>
